{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tr_jEBnh-jv"
      },
      "source": [
        "Title: EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\n",
        "\n",
        "Group Member Names:\n",
        "Kishan Nishad - 200612579\n",
        "Lloyd Ramatshaba - 200617563\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeKSxMvrh-j0"
      },
      "source": [
        "INTRODUCTION:\n",
        "*********************************************************************************************************************\n",
        "AIM:To reproduce the similar result from the paper https://paperswithcode.com/paper/eda-easy-data-augmentation-techniques-for\n",
        "\n",
        "*********************************************************************************************************************\n",
        "Github Repo: https://github.com/jasonwei20/eda_nlp\n",
        "\n",
        "*********************************************************************************************************************\n",
        "DESCRIPTION OF PAPER: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.\n",
        "\n",
        "*********************************************************************************************************************\n",
        "PROBLEM STATEMENT: Can this model have the same accuracy when tested on a different data set that has no relation to its orginal data set.\n",
        "\n",
        "*********************************************************************************************************************\n",
        "CONTEXT OF THE PROBLEM: The context of the problem is to improve the performance of text classification models, particularly convolutional and recurrent neural networks, when working with small datasets. Limited data often leads to overfitting and poor generalization, resulting in suboptimal model accuracy. The challenge is to find a way to enhance model performance without requiring additional data, particularly in scenarios where expanding the dataset is not feasible.\n",
        "\n",
        "\n",
        "*********************************************************************************************************************\n",
        "SOLUTION: To improve text classification models working on small datasets, you can use data augmentation techniques, leverage pre-trained models through transfer learning, and apply regularization methods like dropout and weight regularization to prevent overfitting. Additionally, fine-tuning hyperparameters and using ensemble methods can further enhance model performance. These strategies help improve generalization without needing additional data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77PIPLQ-h-j1"
      },
      "source": [
        "Background\n",
        "*********************************************************************************************************************\n",
        "\n",
        "\n",
        "|Reference|@inproceedings{wei-zou-2019-eda,\n",
        "    title = \"{EDA}: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\",\n",
        "    author = \"Wei, Jason  and\n",
        "      Zou, Kai\",\n",
        "    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)\",\n",
        "    month = nov,\n",
        "    year = \"2019\",\n",
        "    address = \"Hong Kong, China\",\n",
        "    publisher = \"Association for Computational Linguistics\",\n",
        "    url = \"https://www.aclweb.org/anthology/D19-1670\",\n",
        "    pages = \"63\n",
        "\n",
        "    83 --6389\",xplan\n",
        "\n",
        "load_data_tensorflow\n",
        "a| tion|Data\n",
        "\n",
        "Model is not effcient when working with pre-trained models, and bigger data sets.****************\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deODH3tMh-j2"
      },
      "source": [
        "# Implement paper code :\n",
        "*********************************************************************************************************************\n",
        "\n",
        "*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NVxXhfDZLwy"
      },
      "outputs": [],
      "source": [
        "We, as a team, Implemented the code from the paper https://paperswithcode.com/paper/eda-easy-data-augmentation-techniques-for\n",
        "The code include the following: Implementation of original code from the paper https://github.com/jasonwei20/eda_nlp for CNN and RNN models.\n",
        "Furthermore, to get a better result from the original, we also did hyperparameter tuning on the original code for the CNN model to get the best parameters,\n",
        "we rebuilt the CNN models using the best parameters to reach better accuracy.\n",
        "CNN and RNN models have been trained and validated on the SST dataset, which is already in the https://github.com/jasonwei20/eda_nlp/data folder.\n",
        "The final model has been tested on the IMDB dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gkHhku9h-j2"
      },
      "source": [
        "*********************************************************************************************************************\n",
        "### Contribution  Code :\n",
        "*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "UuKvqBuOZLwz",
        "outputId": "d3885a77-0f9f-4b7a-e263-4366ea0a8f24"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 19) (<ipython-input-1-514d65afe31f>, line 19)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-514d65afe31f>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    Accuracy and loss curves for both the training and validation data are plotted to visualize the model's performance over epochs.\u001b[0m\n\u001b[0m                                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 19)\n"
          ]
        }
      ],
      "source": [
        "his code is an end-to-end implementation of a text data augmentation technique combined with a convolutional neural network (CNN) model on the IMDB dataset from scratch just to have better results.\n",
        "\n",
        "The key components are:\n",
        "1. Text Data Augmentation using EDA (Easy Data Augmentation)\n",
        "The code defines several functions (synonym_replacement, random_insertion, random_swap, and random_deletion) that apply various data augmentation techniques to the text, helping to artificially increase the size of the training dataset.\n",
        "These techniques generate new variations of sentences by replacing words with synonyms, randomly inserting synonyms, swapping words within the sentence, or deleting words with a certain probability.\n",
        "The eda() function applies these techniques to a given sentence to generate augmented versions.\n",
        "2. IMDB Dataset Loading and Preprocessing\n",
        "The IMDB movie review dataset is loaded, which contains sequences of integers representing words in reviews.\n",
        "The sequences are padded to ensure uniform input size for the model.\n",
        "A function is provided to decode the encoded reviews back into human-readable text.\n",
        "3. Data Augmentation and Encoding\n",
        "The text augmentation techniques are applied to the decoded reviews, generating additional augmented data.\n",
        "The augmented reviews are then re-encoded into sequences of integers using the word index dictionary.\n",
        "The original and augmented datasets are combined to create a larger training set.\n",
        "4. CNN Model Definition and Training\n",
        "A simple CNN model is defined, consisting of an Embedding layer, a Conv1D layer, and GlobalMaxPooling1D, followed by Dense layers with dropout and L2 regularization to prevent overfitting.\n",
        "The model is compiled with binary cross-entropy loss and trained using the augmented dataset. An EarlyStopping callback is used to halt training when the validation loss stops improving.\n",
        "5. Evaluation and Visualization\n",
        "After training, the model is evaluated on the test set, and the test accuracy is printed.\n",
        "Accuracy and loss curves for both the training and validation data are plotted to visualize the model's performance over epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YdFCgWoh-j3"
      },
      "source": [
        "### Results :\n",
        "*******************************************************************************************************************************\n",
        "\n",
        "\n",
        "#### Observations :\n",
        "*******************************************************************************************************************************\n",
        "*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG3bgSzFZLwz"
      },
      "source": [
        "We did hyperparameter tuning to the original code for the CNN model to get the best parameters and, we rebuilt the CNN models using the best paramters to reach better accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3JVj9dKh-j3"
      },
      "source": [
        "Limitations:\n",
        "EDA mayt no be able tot yield substantial improvements whe\n",
        "using pre-trained models\n",
        ". One study found th t\n",
        "EDA’s improvement was negligible when us ng\n",
        "ULMFit (Shleifer, 2019), and we expect sim lar\n",
        "results for ELMo (Peters et al., 2018) and BERT\n",
        "(Devlin et al., trivial\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATXtFdtBh-j4"
      },
      "source": [
        "References:\n",
        "\n",
        "Wei, J., & Zou, K. (2019). EDA: Easy data augmentation techniques for boosting performance on text classification tasks. arXiv. https://doi.org/10.48550/arXiv.1901.11196"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQnMSAf-h-j4"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}